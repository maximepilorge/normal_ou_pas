# À n'exécuter qu'une seule fois ou lors de la mise à jour des données brutes.
# 1. Chargement des librairies
library(dplyr)
library(lubridate)
library(here)
source(here("global.R"))
dirApp <- "C:/Users/maxp1/Documents/guess_climate"
key_cds <- "9e71b600-abdb-4b74-92f5-94721d1f774f"
cat("Début du pré-calcul des normales climatiques...\n")
# 2. Chargement des données brutes
# Assurez-vous que le nom du fichier et des colonnes correspondent bien aux vôtres.
# D'après votre script de téléchargement, les colonnes sont : ville, date, temperature_max
donnees_brutes <- readRDS(paste0(dirApp, "/data/era5_temperatures_france.rds")) %>%
rename(city = ville, tmax_celsius = temperature_max)
# 3. Ajout des informations temporelles et assignation aux périodes de référence
donnees_augmentees <- donnees_brutes %>%
mutate(
annee = year(date),
jour_annee = yday(date),
# On définit les différentes périodes de référence de 30 ans
periode_ref = case_when(
annee >= 1951 & annee <= 1960 ~ "1951-1960",
annee >= 1951 & annee <= 1980 ~ "1951-1980",
annee >= 1961 & annee <= 1990 ~ "1961-1990",
annee >= 1971 & annee <= 2000 ~ "1971-2000",
annee >= 1981 & annee <= 2010 ~ "1981-2010",
annee >= 1991 & annee <= 2020 ~ "1991-2020",
TRUE ~ NA_character_ # Les années hors de ces périodes ne seront pas utilisées pour les normales
)
) %>%
# On retire les lignes qui n'appartiennent à aucune période de référence
filter(!is.na(periode_ref))
# 4. Calcul des statistiques par ville, par jour de l'année et par période
stats_normales <- donnees_augmentees %>%
group_by(city, jour_annee, periode_ref) %>%
summarise(
t_min = min(tmax_celsius, na.rm = TRUE),
t_q1 = quantile(tmax_celsius, probs = 0.25, na.rm = TRUE),
t_moy = mean(tmax_celsius, na.rm = TRUE),
t_q3 = quantile(tmax_celsius, probs = 0.75, na.rm = TRUE),
t_max = max(tmax_celsius, na.rm = TRUE),
.groups = "drop"
)
# 6. Filtrage final pour retirer les statistiques invalides
#    (issues de jours où toutes les données seraient manquantes)
stats_normales_propres <- stats_normales %>%
filter(
!is.infinite(t_min) &
!is.infinite(t_max) &
!is.nan(t_moy) &
!is.na(t_moy)
)
# 7. Sauvegarde du fichier de statistiques prêt à l'emploi
saveRDS(stats_normales_propres, paste0(dirApp, "/data/stats_normales_precalculees.rds"))
cat("✅ Fichier 'stats_normales_precalculees.rds' (nettoyé) créé avec succès !\n")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
# Script R pour télécharger les données de température ERA5-Land
# Objectif : Télécharger et traiter les températures journalières maximales
# pour 30 villes françaises de 1950 à aujourd'hui, avec une sauvegarde incrémentale.
# --------------------
# 1. Chargement des librairies
# --------------------
# Ces librairies sont essentielles pour les opérations suivantes :
# ecmwfr : Pour interagir avec l'API du service de Copernicus (CDS) afin de télécharger les données.
# dplyr : Pour la manipulation de données (filtrage, regroupement, résumé, etc.).
# tidyr : Pour la réorganisation des données.
# lubridate : Pour faciliter la gestion des dates et des heures.
# ncdf4 : Pour lire les fichiers au format NetCDF, le format de sortie des données ERA5.
# sf : Pour la gestion des données géospatiales, notamment la recherche des points de grille les plus proches des villes.
library(ecmwfr)
library(dplyr)
library(tidyr)
library(lubridate)
library(ncdf4)
library(sf)
library(here)
source(here("global.R"))
dirApp <- "C:/Users/maxp1/Documents/guess_climate"
key_cds <- "9e71b600-abdb-4b74-92f5-94721d1f774f"
# --------------------
# 2. Définition de la fonction principale
# --------------------
# Cette fonction encapsule tout le processus de récupération et de traitement des données.
# Elle peut être appelée avec une liste de villes spécifiques à télécharger, si nécessaire.
recuperer_donnees_era5 <- function(villes_a_telecharger = NULL) {
# Si des villes spécifiques sont fournies, on filtre la liste.
if (!is.null(villes_a_telecharger)) {
villes <- villes %>% filter(ville %in% villes_a_telecharger)
}
# --- Configuration des fichiers et des chemins ---
# Définition du chemin de sauvegarde et des noms de fichiers.
# Utilisation de `file.path` pour une compatibilité multi-plateforme.
path_to_save <- paste0(dirApp, "/data")
dir.create(path_to_save, showWarnings = FALSE) # Crée le répertoire s'il n'existe pas.
nom_fichier_final <- "era5_temperatures_france.rds"
full_path_final <- file.path(path_to_save, nom_fichier_final)
# --- Gestion de la sauvegarde incrémentale ---
# On vérifie si un fichier de données existe déjà.
# Si oui, on le charge pour continuer le téléchargement à partir de la dernière date.
# Si non, on initialise un data frame vide et on commence le téléchargement depuis 1950.
if (file.exists(full_path_final)) {
print(paste("Fichier existant trouvé. Chargement de", nom_fichier_final, "et poursuite du travail."))
df_final_daily_max <- readRDS(full_path_final)
derniere_date_traitee <- max(df_final_daily_max$date, na.rm = TRUE)
print(paste("Dernière date traitée :", derniere_date_traitee))
# On détermine le point de départ pour la nouvelle boucle.
# On ajoute un mois à la dernière date traitée.
date_de_depart <- floor_date(derniere_date_traitee, "month") + months(1)
annee_debut <- year(date_de_depart)
mois_debut <- month(date_de_depart)
} else {
print(paste("Fichier", nom_fichier_final, "non trouvé. Démarrage de zéro."))
df_final_daily_max <- data.frame()
annee_debut <- 1950
mois_debut <- 1
}
# --- Pré-calcul des coordonnées de grille ---
# Cette étape est cruciale car les données ERA5 sont sur une grille.
# On télécharge un petit échantillon de données pour déterminer la grille et
# trouver les points de grille les plus proches de chaque ville.
print("Détermination des points de grille les plus proches pour chaque ville...")
nc_grid_file <- "era5_grid_temp"
full_nc_grid_path <- file.path(path_to_save, nc_grid_file)
request_list_grid <- list(
dataset_short_name = "reanalysis-era5-land",
product_type = "reanalysis",
variable = "2m_temperature",
year = "2023",
month = "01",
day = "01",
time = "00:00",
format = "netcdf",
# On définit une zone géographique qui englobe toutes les villes
area = c(max(villes$latitude) + 1, min(villes$longitude) - 1,
min(villes$latitude) - 1, max(villes$longitude) + 1),
target = paste0(nc_grid_file, ".nc")
)
# Envoi de la requête pour la grille de référence.
wf_request(user = "maxp17.mp@gmail.com",
request_list_grid,
path = path_to_save,
transfer = TRUE,
verbose = FALSE)
# Décompression du fichier téléchargé et renommage pour la suite du traitement.
downloaded_file <- file.path(path_to_save, paste0(nc_grid_file, ".zip"))
if (grepl("\\.zip$", downloaded_file)) {
unzip(zipfile = downloaded_file, exdir = path_to_save)
extracted_nc_file <- file.path(path_to_save, "data_0.nc")
if (file.exists(extracted_nc_file)) {
file.rename(from = extracted_nc_file, to = file.path(path_to_save, paste0(nc_grid_file, ".nc")))
print("Fichier de grille décompressé et renommé avec succès.")
} else {
stop("Erreur : Le fichier 'data_0.nc' n'a pas été trouvé après la décompression.")
}
}
# Lecture du fichier de grille pour extraire les coordonnées.
nc_grid_path <- paste0(full_nc_grid_path, ".nc")
nc_grid <- nc_open(nc_grid_path)
lon_grid <- ncvar_get(nc_grid, "longitude")
lat_grid <- ncvar_get(nc_grid, "latitude")
nc_close(nc_grid)
file.remove(nc_grid_path) # Nettoyage du fichier temporaire.
# Conversion des données de villes et de grille en objets géospatiaux `sf`
# pour trouver les points de grille les plus proches.
sf_villes <- st_as_sf(villes, coords = c("longitude", "latitude"), crs = 4326)
sf_grid <- st_as_sf(expand.grid(lon = lon_grid, lat = lat_grid), coords = c("lon", "lat"), crs = 4326)
# Utilisation de `st_nearest_feature` pour trouver l'index de la grille la plus proche pour chaque ville.
indices_plus_proches <- st_nearest_feature(sf_villes, sf_grid)
# On ajoute les coordonnées de grille correspondantes aux données des villes.
villes_avec_coords_grille <- bind_cols(villes, st_coordinates(sf_grid[indices_plus_proches,])) %>%
rename(lon_grille = X, lat_grille = Y)
print("Début du téléchargement des données ERA5-Land par année et par mois...")
# --- Boucle de téléchargement et de traitement des données ---
# On parcourt les années et les mois à partir du point de départ défini précédemment
# pour télécharger les données incrémentalement.
for (year_to_download in annee_debut:2024) {
# Si on est dans l'année de départ, on commence au mois de départ, sinon on commence en janvier (1).
mois_depart_annee <- ifelse(year_to_download == annee_debut, mois_debut, 1)
for (month_to_download in mois_depart_annee:12) {
# On s'assure de ne pas essayer de télécharger des données pour un futur mois de l'année en cours.
if (year_to_download == year(Sys.Date()) && month_to_download > month(Sys.Date())) {
next
}
print(paste("Téléchargement et traitement des données pour l'année :", year_to_download, ", mois :", month_to_download))
# Nom de fichier unique pour chaque mois, pour éviter les conflits.
nc_file_name <- paste0("era5_data_raw_", year_to_download, "_", month_to_download)
full_nc_file_path <- file.path(path_to_save, nc_file_name)
# Création de la liste de requête pour le mois et l'année en cours.
request_list <- list(
dataset_short_name = "reanalysis-era5-land",
product_type = "reanalysis",
variable = "2m_temperature",
year = as.character(year_to_download),
month = as.character(month_to_download),
day = as.character(1:31),
time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00",
"06:00", "07:00", "08:00", "09:00", "10:00", "11:00",
"12:00", "13:00", "14:00", "15:00", "16:00", "17:00",
"18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
format = "netcdf",
target = nc_file_name,
area = c(max(villes$latitude) + 1, min(villes$longitude) - 1,
min(villes$latitude) - 1, max(villes$longitude) + 1)
)
# Téléchargement des données via l'API.
wf_request(user = "maxp17.mp@gmail.com",
request = request_list,
path = path_to_save,
transfer = TRUE,
verbose = FALSE)
# Décompression et renommage du fichier téléchargé.
downloaded_zip_file <- file.path(path_to_save, paste0(nc_file_name, ".zip"))
if (file.exists(downloaded_zip_file)) {
unzip(zipfile = downloaded_zip_file, exdir = path_to_save)
extracted_nc_file <- file.path(path_to_save, "data_0.nc")
if (file.exists(extracted_nc_file)) {
file.rename(from = extracted_nc_file, to = paste0(full_nc_file_path, ".nc"))
print(paste("Fichier décompressé et renommé :", paste0(full_nc_file_path, ".nc")))
} else {
stop(paste("Erreur : Le fichier 'data_0.nc' n'a pas été trouvé après la décompression de", downloaded_zip_file))
}
}
# --- Lecture et traitement des données NetCDF ---
# On ouvre le fichier, extrait les données, les nettoie et les prépare pour l'analyse.
nc_path <- paste0(full_nc_file_path, ".nc")
if (file.exists(nc_path)) {
nc <- nc_open(nc_path)
# Extraction des variables (longitude, latitude, température)
lon <- ncvar_get(nc, "longitude")
lat <- ncvar_get(nc, "latitude")
t2m <- ncvar_get(nc, "t2m")
nc_close(nc) # Fermeture du fichier NetCDF.
file.remove(nc_path) # Nettoyage du fichier NetCDF temporaire.
if (file.exists(downloaded_zip_file)) file.remove(downloaded_zip_file) # Nettoyage du fichier zip.
# Création d'une séquence de dates et d'heures pour chaque observation.
month_str <- sprintf("%02d", month_to_download)
day_str <- sprintf("%02d", 1)
date_string <- paste(year_to_download, month_str, day_str, "00:00:00", sep = "-")
start_date_time <- as.POSIXct(date_string, tz = "UTC")
total_timesteps <- length(t2m) / (length(lon) * length(lat))
dates <- seq.POSIXt(from = start_date_time, by = "hour", length.out = total_timesteps)
# Conversion des températures de Kelvin en Celsius.
t2m_degC <- t2m - 273.15
# Création d'un data frame brut avec toutes les données horaires de la grille.
df_raw <- as_tibble(expand.grid(lon = lon, lat = lat, time = dates)) %>%
mutate(t2m = as.vector(t2m_degC))
# Jointure des données de température avec les villes, en utilisant les coordonnées de grille.
df_final <- df_raw %>%
inner_join(villes_avec_coords_grille, by = c("lon" = "lon_grille", "lat" = "lat_grille")) %>%
select(ville, date = time, temperature = t2m)
# Calcul de la température maximale journalière pour chaque ville.
df_monthly_daily_max <- df_final %>%
mutate(date = as_date(date)) %>%
group_by(ville, date) %>%
summarise(temperature_max = max(temperature, na.rm = TRUE), .groups = "drop")
# --- Sauvegarde incrémentale ---
# On ajoute les nouvelles données au data frame final et on le sauvegarde.
df_final_daily_max <- bind_rows(df_final_daily_max, df_monthly_daily_max)
saveRDS(df_final_daily_max, full_path_final)
print(paste("Données pour le mois de", month_to_download, "sauvegardées avec succès dans le fichier :", nom_fichier_final))
} else {
warning(paste("Le fichier NetCDF n'a pas été trouvé pour l'année", year_to_download, "et le mois", month_to_download))
}
}
}
print("Traitement terminé. Le fichier final est à jour.")
return(df_final_daily_max)
}
# --------------------
# 3. Exécution du script
# --------------------
# On définit d'abord la clé d'API et l'utilisateur pour le service Copernicus.
wf_set_key(key = key_cds, user = "maxp17.mp@gmail.com")
# Appel de la fonction pour démarrer le processus de récupération des données.
recuperer_donnees_era5()
runApp()
# Script R pour télécharger les données de température ERA5-Land
# Objectif : Télécharger et traiter les températures journalières maximales
# pour 30 villes françaises de 1950 à aujourd'hui, avec une sauvegarde incrémentale.
# --------------------
# 1. Chargement des librairies
# --------------------
# Ces librairies sont essentielles pour les opérations suivantes :
# ecmwfr : Pour interagir avec l'API du service de Copernicus (CDS) afin de télécharger les données.
# dplyr : Pour la manipulation de données (filtrage, regroupement, résumé, etc.).
# tidyr : Pour la réorganisation des données.
# lubridate : Pour faciliter la gestion des dates et des heures.
# ncdf4 : Pour lire les fichiers au format NetCDF, le format de sortie des données ERA5.
# sf : Pour la gestion des données géospatiales, notamment la recherche des points de grille les plus proches des villes.
library(ecmwfr)
library(dplyr)
library(tidyr)
library(lubridate)
library(ncdf4)
library(sf)
library(here)
source(here("global.R"))
dirApp <- "C:/Users/maxp1/Documents/guess_climate"
key_cds <- "9e71b600-abdb-4b74-92f5-94721d1f774f"
# --------------------
# 2. Définition de la fonction principale
# --------------------
# Cette fonction encapsule tout le processus de récupération et de traitement des données.
# Elle peut être appelée avec une liste de villes spécifiques à télécharger, si nécessaire.
recuperer_donnees_era5 <- function(villes_a_telecharger = NULL) {
# Si des villes spécifiques sont fournies, on filtre la liste.
if (!is.null(villes_a_telecharger)) {
villes <- villes %>% filter(ville %in% villes_a_telecharger)
}
# --- Configuration des fichiers et des chemins ---
# Définition du chemin de sauvegarde et des noms de fichiers.
# Utilisation de `file.path` pour une compatibilité multi-plateforme.
path_to_save <- paste0(dirApp, "/data")
dir.create(path_to_save, showWarnings = FALSE) # Crée le répertoire s'il n'existe pas.
nom_fichier_final <- "era5_temperatures_france.rds"
full_path_final <- file.path(path_to_save, nom_fichier_final)
# --- Gestion de la sauvegarde incrémentale ---
# On vérifie si un fichier de données existe déjà.
# Si oui, on le charge pour continuer le téléchargement à partir de la dernière date.
# Si non, on initialise un data frame vide et on commence le téléchargement depuis 1950.
if (file.exists(full_path_final)) {
print(paste("Fichier existant trouvé. Chargement de", nom_fichier_final, "et poursuite du travail."))
df_final_daily_max <- readRDS(full_path_final)
derniere_date_traitee <- max(df_final_daily_max$date, na.rm = TRUE)
print(paste("Dernière date traitée :", derniere_date_traitee))
# On détermine le point de départ pour la nouvelle boucle.
# On ajoute un mois à la dernière date traitée.
date_de_depart <- floor_date(derniere_date_traitee, "month") + months(1)
annee_debut <- year(date_de_depart)
mois_debut <- month(date_de_depart)
} else {
print(paste("Fichier", nom_fichier_final, "non trouvé. Démarrage de zéro."))
df_final_daily_max <- data.frame()
annee_debut <- 1950
mois_debut <- 1
}
# --- Pré-calcul des coordonnées de grille ---
# Cette étape est cruciale car les données ERA5 sont sur une grille.
# On télécharge un petit échantillon de données pour déterminer la grille et
# trouver les points de grille les plus proches de chaque ville.
print("Détermination des points de grille les plus proches pour chaque ville...")
nc_grid_file <- "era5_grid_temp"
full_nc_grid_path <- file.path(path_to_save, nc_grid_file)
request_list_grid <- list(
dataset_short_name = "reanalysis-era5-land",
product_type = "reanalysis",
variable = "2m_temperature",
year = "2023",
month = "01",
day = "01",
time = "00:00",
format = "netcdf",
# On définit une zone géographique qui englobe toutes les villes
area = c(max(villes$latitude) + 1, min(villes$longitude) - 1,
min(villes$latitude) - 1, max(villes$longitude) + 1),
target = paste0(nc_grid_file, ".nc")
)
# Envoi de la requête pour la grille de référence.
wf_request(user = "maxp17.mp@gmail.com",
request_list_grid,
path = path_to_save,
transfer = TRUE,
verbose = FALSE)
# Décompression du fichier téléchargé et renommage pour la suite du traitement.
downloaded_file <- file.path(path_to_save, paste0(nc_grid_file, ".zip"))
if (grepl("\\.zip$", downloaded_file)) {
unzip(zipfile = downloaded_file, exdir = path_to_save)
extracted_nc_file <- file.path(path_to_save, "data_0.nc")
if (file.exists(extracted_nc_file)) {
file.rename(from = extracted_nc_file, to = file.path(path_to_save, paste0(nc_grid_file, ".nc")))
print("Fichier de grille décompressé et renommé avec succès.")
} else {
stop("Erreur : Le fichier 'data_0.nc' n'a pas été trouvé après la décompression.")
}
}
# Lecture du fichier de grille pour extraire les coordonnées.
nc_grid_path <- paste0(full_nc_grid_path, ".nc")
nc_grid <- nc_open(nc_grid_path)
lon_grid <- ncvar_get(nc_grid, "longitude")
lat_grid <- ncvar_get(nc_grid, "latitude")
nc_close(nc_grid)
file.remove(nc_grid_path) # Nettoyage du fichier temporaire.
# Conversion des données de villes et de grille en objets géospatiaux `sf`
# pour trouver les points de grille les plus proches.
sf_villes <- st_as_sf(villes, coords = c("longitude", "latitude"), crs = 4326)
sf_grid <- st_as_sf(expand.grid(lon = lon_grid, lat = lat_grid), coords = c("lon", "lat"), crs = 4326)
# Utilisation de `st_nearest_feature` pour trouver l'index de la grille la plus proche pour chaque ville.
indices_plus_proches <- st_nearest_feature(sf_villes, sf_grid)
# On ajoute les coordonnées de grille correspondantes aux données des villes.
villes_avec_coords_grille <- bind_cols(villes, st_coordinates(sf_grid[indices_plus_proches,])) %>%
rename(lon_grille = X, lat_grille = Y)
print("Début du téléchargement des données ERA5-Land par année et par mois...")
# --- Boucle de téléchargement et de traitement des données ---
# On parcourt les années et les mois à partir du point de départ défini précédemment
# pour télécharger les données incrémentalement.
for (year_to_download in annee_debut:2024) {
# Si on est dans l'année de départ, on commence au mois de départ, sinon on commence en janvier (1).
mois_depart_annee <- ifelse(year_to_download == annee_debut, mois_debut, 1)
for (month_to_download in mois_depart_annee:12) {
# On s'assure de ne pas essayer de télécharger des données pour un futur mois de l'année en cours.
if (year_to_download == year(Sys.Date()) && month_to_download > month(Sys.Date())) {
next
}
print(paste("Téléchargement et traitement des données pour l'année :", year_to_download, ", mois :", month_to_download))
# Nom de fichier unique pour chaque mois, pour éviter les conflits.
nc_file_name <- paste0("era5_data_raw_", year_to_download, "_", month_to_download)
full_nc_file_path <- file.path(path_to_save, nc_file_name)
# Création de la liste de requête pour le mois et l'année en cours.
request_list <- list(
dataset_short_name = "reanalysis-era5-land",
product_type = "reanalysis",
variable = "2m_temperature",
year = as.character(year_to_download),
month = as.character(month_to_download),
day = as.character(1:31),
time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00",
"06:00", "07:00", "08:00", "09:00", "10:00", "11:00",
"12:00", "13:00", "14:00", "15:00", "16:00", "17:00",
"18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
format = "netcdf",
target = nc_file_name,
area = c(max(villes$latitude) + 1, min(villes$longitude) - 1,
min(villes$latitude) - 1, max(villes$longitude) + 1)
)
# Téléchargement des données via l'API.
wf_request(user = "maxp17.mp@gmail.com",
request = request_list,
path = path_to_save,
transfer = TRUE,
verbose = FALSE)
# Décompression et renommage du fichier téléchargé.
downloaded_zip_file <- file.path(path_to_save, paste0(nc_file_name, ".zip"))
if (file.exists(downloaded_zip_file)) {
unzip(zipfile = downloaded_zip_file, exdir = path_to_save)
extracted_nc_file <- file.path(path_to_save, "data_0.nc")
if (file.exists(extracted_nc_file)) {
file.rename(from = extracted_nc_file, to = paste0(full_nc_file_path, ".nc"))
print(paste("Fichier décompressé et renommé :", paste0(full_nc_file_path, ".nc")))
} else {
stop(paste("Erreur : Le fichier 'data_0.nc' n'a pas été trouvé après la décompression de", downloaded_zip_file))
}
}
# --- Lecture et traitement des données NetCDF ---
# On ouvre le fichier, extrait les données, les nettoie et les prépare pour l'analyse.
nc_path <- paste0(full_nc_file_path, ".nc")
if (file.exists(nc_path)) {
nc <- nc_open(nc_path)
# Extraction des variables (longitude, latitude, température)
lon <- ncvar_get(nc, "longitude")
lat <- ncvar_get(nc, "latitude")
t2m <- ncvar_get(nc, "t2m")
nc_close(nc) # Fermeture du fichier NetCDF.
file.remove(nc_path) # Nettoyage du fichier NetCDF temporaire.
if (file.exists(downloaded_zip_file)) file.remove(downloaded_zip_file) # Nettoyage du fichier zip.
# Création d'une séquence de dates et d'heures pour chaque observation.
month_str <- sprintf("%02d", month_to_download)
day_str <- sprintf("%02d", 1)
date_string <- paste(year_to_download, month_str, day_str, "00:00:00", sep = "-")
start_date_time <- as.POSIXct(date_string, tz = "UTC")
total_timesteps <- length(t2m) / (length(lon) * length(lat))
dates <- seq.POSIXt(from = start_date_time, by = "hour", length.out = total_timesteps)
# Conversion des températures de Kelvin en Celsius.
t2m_degC <- t2m - 273.15
# Création d'un data frame brut avec toutes les données horaires de la grille.
df_raw <- as_tibble(expand.grid(lon = lon, lat = lat, time = dates)) %>%
mutate(t2m = as.vector(t2m_degC))
# Jointure des données de température avec les villes, en utilisant les coordonnées de grille.
df_final <- df_raw %>%
inner_join(villes_avec_coords_grille, by = c("lon" = "lon_grille", "lat" = "lat_grille")) %>%
select(ville, date = time, temperature = t2m)
# Calcul de la température maximale journalière pour chaque ville.
df_monthly_daily_max <- df_final %>%
mutate(date = as_date(date)) %>%
group_by(ville, date) %>%
summarise(temperature_max = max(temperature, na.rm = TRUE), .groups = "drop")
# --- Sauvegarde incrémentale ---
# On ajoute les nouvelles données au data frame final et on le sauvegarde.
df_final_daily_max <- bind_rows(df_final_daily_max, df_monthly_daily_max)
saveRDS(df_final_daily_max, full_path_final)
print(paste("Données pour le mois de", month_to_download, "sauvegardées avec succès dans le fichier :", nom_fichier_final))
} else {
warning(paste("Le fichier NetCDF n'a pas été trouvé pour l'année", year_to_download, "et le mois", month_to_download))
}
}
}
print("Traitement terminé. Le fichier final est à jour.")
return(df_final_daily_max)
}
# --------------------
# 3. Exécution du script
# --------------------
# On définit d'abord la clé d'API et l'utilisateur pour le service Copernicus.
wf_set_key(key = key_cds, user = "maxp17.mp@gmail.com")
# Appel de la fonction pour démarrer le processus de récupération des données.
recuperer_donnees_era5()
